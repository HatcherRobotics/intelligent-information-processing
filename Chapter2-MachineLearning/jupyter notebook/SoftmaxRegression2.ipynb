{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateData(n):\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    for i in range(n):\n",
    "        for j in range(8):\n",
    "            train_x.append(random.normalvariate(i*10,0.01))\n",
    "            train_y.append(i)\n",
    "        for k in range(2):\n",
    "            test_x.append(random.normalvariate(i*10,0.01))\n",
    "            test_y.append(i)\n",
    "    train_x  = torch.Tensor(train_x)\n",
    "    train_y = torch.Tensor(train_y)\n",
    "    test_x = torch.Tensor(test_x)\n",
    "    test_y = torch.Tensor(test_y)\n",
    "    return train_x,train_y,test_x,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,train_y,test_x,test_y = GenerateData(10)\n",
    "import torch.utils.data as Data\n",
    "batch_size = 8# 将训练数据的特征和标签组合\n",
    "train_dataset,test_dataset = Data.TensorDataset(train_x, train_y),Data.TensorDataset(test_x, test_y)\n",
    "# 把 dataset 放入 DataLoader\n",
    "train_iter = Data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "test_iter = Data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_SoftmaxNet(nn.Module):\n",
    "    def __init__(self,num_inputs,num_outputs):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.w = nn.Parameter(torch.normal(0, 0.01, size=(self.num_inputs, self.num_outputs), requires_grad=True))\n",
    "        self.b = nn.Parameter(torch.zeros(self.num_outputs, requires_grad=True))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        for param in self.parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def softmax(self,x):\n",
    "        X_exp = torch.exp(x)\n",
    "        partition = X_exp.sum(1, keepdim=True)\n",
    "        return X_exp / partition  # 这里应用了广播机制\n",
    "    \n",
    "    def forward(self,x):\n",
    "        initial_output = self.sigmoid(torch.mm(x.view(-1,self.num_inputs),self.w)+self.b)\n",
    "        softmax_output = self.softmax(initial_output)\n",
    "        return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return - torch.log(y_hat.gather(1, y.view(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, loss):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    test_l_sum = 0.0\n",
    "    for X, y in data_iter:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        y_hat = net(X)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        l = loss(y_hat, y).sum()\n",
    "        test_l_sum += l.item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum/n, test_l_sum/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,train_iter,test_iter,loss,num_epochs,optimizer_w,optimizer_b):\n",
    "    train_loss=[]\n",
    "    test_loss=[]\n",
    "    train_accuracy=[]\n",
    "    test_accuracy=[]\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n=0.0,0.0,0\n",
    "        for X,y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device) \n",
    "            y_hat = net(X)\n",
    "            w,b = net.parameters()\n",
    "            y_hat = y_hat.squeeze()\n",
    "            l=loss(y_hat,y).sum()\n",
    "            optimizer_w.zero_grad()\n",
    "            optimizer_b.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer_w.step()\n",
    "            optimizer_b.step()\n",
    "            train_l_sum+=l.item()\n",
    "            train_acc_sum+=(y_hat.argmax(dim=1)==y).sum().item()\n",
    "            n+=y.shape[0]\n",
    "        test_acc,test_l = evaluate_accuracy(test_iter,net,loss)\n",
    "        train_loss.append(train_l_sum/n)\n",
    "        test_loss.append(test_l)\n",
    "        train_accuracy.append(train_acc_sum/n)\n",
    "        test_accuracy.append(test_acc)\n",
    "        print('epoch%d,loss%.4f,train acc %3f,test acc %.3f'%(epoch+1,train_l_sum/n,train_acc_sum/n,test_acc))\n",
    "    for params in net.parameters():\n",
    "        print(params)\n",
    "    return train_loss,test_loss,train_accuracy,test_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
